---
title: Lab 8 Â· Explore Prompting
description: Exercise 1 - Basic model inference and comparison
---

# Getting Started with Model Inference

This section walks you through the first Jupyter notebook exercise, where you'll learn to perform basic operations with Azure AI Foundry models programmatically. We'll explore how to create a model inference client, send prompts to different models, and analyze their responses.

## Accessing the Notebook

1. Open the file explorer in your VS Code environment
2. Navigate to the `notebooks` folder
3. Open `01-getting-started.ipynb`

## Understanding the Core Library

The primary library we'll use is `azure-ai-generative`, which provides the Python SDK for Azure AI Foundry services. The key components we'll use include:

- `ModelInferenceClient`: The main client for making inference calls to models
- `AzureKeyCredential`: For handling API key authentication

```python
from azure.ai.generative import ModelInferenceClient
from azure.core.credentials import AzureKeyCredential
```

## Creating the Model Inference Client

The first step in working with Azure AI Foundry models is to create a client that handles the communication with the service:

```python
# Load environment variables
load_dotenv()

# Get Azure AI Foundry endpoint and key
endpoint = os.getenv("AZURE_AI_FOUNDRY_ENDPOINT")
api_key = os.getenv("AZURE_AI_FOUNDRY_API_KEY")

# Create Model Inference Client
client = ModelInferenceClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(api_key)
)
```

This client will be used for all model interactions throughout the notebook.

## Understanding Model IDs

Azure AI Foundry uses specific IDs to identify models. In this workshop, we focus on three key models:

```python
# Define model IDs
claude_sonnet_id = "anthropic-claude-3-sonnet-20240229-v1:0"  # Claude (o1)
claude_haiku_id = "anthropic-claude-3-haiku-20240307-v1:0"    # Claude 3 Mini (o3-mini)
gpt4o_mini_id = "gpt-4o-mini-2024-07-18"                     # GPT-4o Mini
```

These IDs include the model name and specific version, ensuring you're accessing the exact model version you need.

## Making Your First Model Call

The notebook demonstrates a basic model call using the `complete` method:

```python
def call_model(model_id, prompt, temperature=0.7, reasoning_level=None):
    # Prepare request
    request = {
        "prompt": prompt,
        "temperature": temperature,
        "max_tokens": 1000
    }
    
    # Add reasoning level if provided
    if reasoning_level and model_id.startswith("anthropic"):
        request["reasoning"] = reasoning_level
    
    # Call the model
    response = client.complete(model_id, request)
    
    # Return response and metrics
    return {
        "response": response.text,
        "metrics": {
            "elapsed_time": elapsed_time,
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.prompt_tokens + response.usage.completion_tokens
        }
    }
```

This function:
1. Creates a request with your prompt and parameters
2. Adds a reasoning level if specified (for Claude models)
3. Makes the API call
4. Returns the response text and usage metrics

## Key Parameters for Model Calls

When making model calls, several parameters influence the response:

| Parameter | Description | Typical Values |
|-----------|-------------|----------------|
| `prompt` | The text prompt to send to the model | Any text string |
| `temperature` | Controls randomness/creativity | 0.0 (deterministic) to 1.0 (creative) |
| `max_tokens` | Maximum response length | 100-2000 depending on need |
| `reasoning` | Reasoning level (Claude only) | "low", "medium", "high" |

## Exercises in the Notebook

The notebook guides you through several exercises:

### 1. Simple Question Exercise

This tests models on straightforward factual questions where reasoning isn't necessary:
```python
simple_question = "What is the capital of France?"
```

### 2. Logic Problem Exercise

This evaluates models on problems requiring multi-step reasoning:
```python
logic_problem = """
A logistics company needs to determine the most efficient delivery route between 5 cities.
The distances between cities (in kilometers) are as follows:
...
"""
```

### 3. Reasoning Level Comparison

This explores how different reasoning levels affect Claude's responses:
```python
reasoning_levels = ["low", "medium", "high"]
```

## Analyzing Results

The notebook includes code to compare results across models:

```python
def print_comparison_table(results, problem_type):
    print(f"\n{problem_type} Comparison")
    print("-" * 80)
    print(f"{'Model':<20} {'Time (s)':<12} {'Prompt Tokens':<15} {'Completion Tokens':<18} {'Total Tokens':<12}")
    # ...
```

This allows you to analyze:
- Response times across models
- Token usage differences
- How complexity affects performance

## Key Code Patterns to Apply

As you work through the notebook, pay attention to these reusable patterns:

1. **Parameter Handling**: How to customize model requests with different parameters
2. **Response Processing**: Extracting and analyzing response content and metadata
3. **Comparative Analysis**: Techniques for comparing model performance
4. **Error Handling**: Handling potential errors in model calls

## Extending the Exercise

After completing the guided exercises, consider these extensions:

1. Try different temperature settings to see how they affect responses
2. Create your own complex reasoning problem and test all models
3. Implement a function that recommends the optimal model based on prompt characteristics
4. Measure the correlation between prompt complexity and response time

## Next Steps

After completing this notebook, you'll have a solid foundation in programmatically working with Azure AI Foundry models. The next notebook will build on these skills to explore more complex scenarios and in-depth performance analysis.