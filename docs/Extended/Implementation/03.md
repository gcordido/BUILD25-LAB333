---
title: Lab 10 Â· Model Chaining
description: Implement a system where reasoning models plan and GPT models execute
---

# Model Orchestration

This section explores strategies for effective orchestration of multiple AI models in a single application. We'll implement a hierarchical architecture where reasoning models handle complex planning while more efficient models execute simpler tasks.

## The Model Orchestration Concept

Model orchestration involves:

1. Routing tasks to the most appropriate model based on task complexity
2. Coordinating multiple models working together on a single problem
3. Managing information flow between models
4. Optimizing for both performance and cost

## Orchestration Patterns for Reasoning Models

### 1. Planning-Execution Pattern

In this pattern:
- **Reasoning Model (Planner)**: Develops the high-level strategy and breaks down complex problems
- **Efficient Model (Executor)**: Handles routine tasks following the plan

```python
def process_catering_order(order):
    # Step 1: Use reasoning model to create a plan
    planning_prompt = f"""
    As a catering operations expert, create a detailed plan for executing this order.
    Break down the process into specific tasks, noting which require complex decision-making
    versus which are routine calculations or transformations.
    
    Order details:
    {json.dumps(order, indent=2)}
    
    Your plan should specify:
    1. The sequence of major operations needed
    2. Which operations require sophisticated reasoning
    3. Which operations are straightforward calculations
    4. What information needs to be passed between steps
    5. Verification checks to ensure quality
    
    Format your response as a JSON execution plan.
    """
    
    # Use Claude (o1) with high reasoning for complex planning
    plan_response = client.complete(
        claude_sonnet_id,
        {
            "prompt": planning_prompt,
            "temperature": 0.2,
            "max_tokens": 2000,
            "reasoning": "high"
        }
    )
    
    execution_plan = json.loads(plan_response.text)
    
    # Step 2: Execute the plan using appropriate models for each step
    results = {}
    for step in execution_plan["steps"]:
        if step["complexity"] == "high":
            # Use reasoning model for complex steps
            results[step["id"]] = execute_complex_step(step, order, results)
        else:
            # Use efficient model for simpler steps
            results[step["id"]] = execute_simple_step(step, order, results)
    
    return combine_results(results)
```

### 2. Hierarchical Decision Tree

This pattern creates a tree of models with different capabilities:

```
                   [Strategic Planner]
                      Claude (o1)
                     /            \
       [Resource Planner]      [Timeline Planner]
       Claude 3 Mini           Claude 3 Mini
        /         \                /       \
[Ingredient    [Equipment      [Recipe   [Service
 Calculator]    Allocator]     Scheduler]  Coordinator]
 GPT-4o Mini    GPT-4o Mini    GPT-4o Mini GPT-4o Mini
```

Implementation example:

```python
def hierarchical_planning(event):
    # Level 1: Strategic planning with Claude (o1)
    strategic_plan = strategic_planning_model(event)
    
    # Level 2: Domain-specific planning with Claude 3 Mini
    resource_plan = resource_planning_model(strategic_plan["resources"])
    timeline_plan = timeline_planning_model(strategic_plan["timeline"])
    
    # Level 3: Tactical execution with GPT-4o Mini
    ingredients = ingredient_calculation_model(resource_plan["ingredients"])
    equipment = equipment_allocation_model(resource_plan["equipment"])
    schedule = recipe_scheduling_model(timeline_plan["recipes"])
    service = service_coordination_model(timeline_plan["service"])
    
    # Combine all outputs into final plan
    return {
        "strategic_overview": strategic_plan["overview"],
        "resources": {
            "ingredients": ingredients,
            "equipment": equipment
        },
        "timeline": {
            "preparation": schedule,
            "service": service
        }
    }
```

### 3. Iterative Refinement Pattern

This pattern iteratively improves solutions:

```python
def generate_catering_plan(order, max_iterations=3):
    # Initial plan generation with reasoning model
    current_plan = generate_initial_plan(order)
    
    for i in range(max_iterations):
        # Evaluate plan quality
        evaluation = evaluate_plan(current_plan, order)
        
        # Stop if plan meets quality threshold
        if evaluation["quality_score"] > 0.9:
            return current_plan
        
        # Use reasoning model to create improvement suggestions
        refinement_prompt = f"""
        Review this catering plan and identify specific improvements:
        {json.dumps(current_plan, indent=2)}
        
        Order requirements:
        {json.dumps(order, indent=2)}
        
        Current evaluation:
        {json.dumps(evaluation, indent=2)}
        
        Suggest specific improvements to address these issues.
        Prioritize the most critical problems first.
        """
        
        refinement_response = client.complete(
            claude_sonnet_id,
            {
                "prompt": refinement_prompt,
                "temperature": 0.3,
                "max_tokens": 1000,
                "reasoning": "high"
            }
        )
        
        # Update plan based on suggestions
        current_plan = apply_improvements(current_plan, refinement_response.text)
    
    return current_plan
```

## Model Selection Strategy

For our catering application, we'll implement these model selection guidelines:

| Task Type | Recommended Model | Reasoning Level | Temperature |
|-----------|------------------|----------------|-------------|
| Complex planning | Claude (o1) | High | 0.1-0.3 |
| Constraint solving | Claude (o1) | Medium-High | 0.1-0.2 |
| Recipe scaling | Claude 3 Mini | Medium | 0.2-0.4 |
| Timeline creation | Claude (o1) | Medium | 0.1-0.3 |
| Ingredient consolidation | Claude 3 Mini | Low-Medium | 0.2-0.4 |
| Unit conversion | GPT-4o Mini | N/A | 0.0-0.1 |
| Text formatting | GPT-4o Mini | N/A | 0.3-0.5 |

## Implementing Model Orchestration

Here's a practical implementation for our catering application:

```python
class ModelOrchestrator:
    def __init__(self, client):
        self.client = client
        
        # Define model IDs
        self.planner_model = "anthropic-claude-3-sonnet-20240229-v1:0"  # Claude (o1)
        self.mid_tier_model = "anthropic-claude-3-haiku-20240307-v1:0"  # Claude 3 Mini
        self.efficient_model = "gpt-4o-mini-2024-07-18"                # GPT-4o Mini
    
    def route_task(self, task_type, data, context=None):
        """Route a task to the appropriate model based on task type"""
        
        task_routing = {
            "strategic_planning": (self.planner_model, "high", 0.2),
            "constraint_solving": (self.planner_model, "high", 0.1),
            "recipe_scaling": (self.mid_tier_model, "medium", 0.3),
            "timeline_creation": (self.planner_model, "medium", 0.2),
            "ingredient_consolidation": (self.mid_tier_model, "medium", 0.3),
            "unit_conversion": (self.efficient_model, None, 0.0),
            "text_formatting": (self.efficient_model, None, 0.4)
        }
        
        if task_type not in task_routing:
            raise ValueError(f"Unknown task type: {task_type}")
        
        model_id, reasoning_level, temperature = task_routing[task_type]
        
        # Get the task-specific prompt template
        prompt = self.get_prompt_template(task_type, data, context)
        
        # Prepare request
        request = {
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": 2000
        }
        
        # Add reasoning level if applicable
        if reasoning_level and model_id.startswith("anthropic"):
            request["reasoning"] = reasoning_level
        
        # Call the model
        response = self.client.complete(model_id, request)
        
        # Process the response based on task type
        return self.process_response(task_type, response.text)
    
    def get_prompt_template(self, task_type, data, context):
        """Get the appropriate prompt template for the task type"""
        # Implementation of task-specific prompt templates
        pass
    
    def process_response(self, task_type, response_text):
        """Process the model response based on task type"""
        # Implementation of task-specific response processing
        pass
```

## Error Handling in Model Orchestration

Robust model orchestration requires comprehensive error handling:

```python
def execute_with_fallback(orchestrator, primary_task, fallback_task, data, context=None):
    """Execute a task with fallback to a different model if it fails"""
    try:
        result = orchestrator.route_task(primary_task, data, context)
        
        # Validate result
        is_valid, issues = validate_result(primary_task, result)
        if is_valid:
            return result
        
        # If validation fails, log issues and try fallback
        logger.warning(f"Primary task {primary_task} produced invalid result: {issues}")
        result = orchestrator.route_task(fallback_task, data, context)
        
        # Validate fallback result
        is_valid, issues = validate_result(fallback_task, result)
        if is_valid:
            return result
        
        # If fallback also fails, raise exception
        raise ValueError(f"Both primary and fallback tasks failed: {issues}")
        
    except Exception as e:
        logger.error(f"Error executing {primary_task}: {str(e)}")
        # Try fallback task if primary fails
        return orchestrator.route_task(fallback_task, data, context)
```

## Performance and Cost Optimization

To optimize the performance and cost of our orchestrated system:

1. **Cache Common Operations**: Store results of frequent operations
2. **Batch Similar Requests**: Combine similar tasks to reduce API calls
3. **Progressive Detail**: Start with rough plans and add detail only when needed
4. **Asynchronous Processing**: Use async calls for independent operations
5. **Resource-Aware Routing**: Consider current system load when routing tasks

```python
# Example of asynchronous model orchestration
async def process_recipes_async(orchestrator, recipes, guests):
    # Process multiple recipes in parallel
    tasks = []
    for recipe in recipes:
        task = asyncio.create_task(
            orchestrator.route_task("recipe_scaling", recipe, {"guests": guests})
        )
        tasks.append(task)
    
    # Wait for all tasks to complete
    scaled_recipes = await asyncio.gather(*tasks)
    return scaled_recipes
```

## Next Steps

In the next section, we'll implement the Recipe Processing component of our catering application, applying these orchestration patterns to handle recipe data and serving adjustments efficiently.