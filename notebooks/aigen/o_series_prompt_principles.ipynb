{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e69c292",
   "metadata": {},
   "source": [
    "# Prompt Engineering Principles for OpenAI Reasoning Models (o4-mini & o1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55982c4",
   "metadata": {},
   "source": [
    "\n",
    "## Principles Overview\n",
    "\n",
    "| Prompting Principle | What it means for **o-series reasoning models (o4-mini & o1)** |\n",
    "|---|---|\n",
    "| **1 ‚Äî Use a `developer` message (not system) and give a clear role** | The newest reasoning models treat a *system* message as a *developer* message; mixing both is discouraged. Put the high‚Äëlevel instruction in a single `role:\"developer\"` message, e.g., ‚ÄúYou are an expert tax lawyer‚Ä¶‚Äù. |\n",
    "| **2 ‚Äî Keep prompts simple & direct ‚Äî don‚Äôt force chain‚Äëof‚Äëthought** | Reasoning models already ‚Äúthink‚Äù internally. Over‚Äëspecifying (‚Äúthink step‚Äëby‚Äëstep‚Ä¶ explain every thought‚Äù) wastes tokens and can leak private reasoning. Ask only for the final answer unless you truly need a public explanation. |\n",
    "| **3 ‚Äî Use explicit delimiters to structure input** | Wrap long passages, code, or multi‚Äëpart instructions in clear fences (` ````, `<doc>‚Ä¶</doc>`, Markdown headings). Delimiters help the model parse sections correctly and reduce mis‚Äëinterpretation. |\n",
    "| **4 ‚Äî Supply *only* the relevant context** | With 200‚ÄØk‚Äëtoken windows you can paste huge docs‚Äîbut you *shouldn‚Äôt*. Include the minimal excerpts the task needs so the model focuses on the right evidence and stays concise. |\n",
    "| **5 ‚Äî Decompose or iterate instead of one giant ask (try zero‚Äëshot first)** | Start with a straightforward version, examine the answer, then refine or break the workflow into numbered sub‚Äëtasks. This lets the model reason deeply on each step and saves tokens. |\n",
    "| **6 ‚Äî Specify output format & boundaries** | Tell the model exactly *how* to answer (JSON schema, Markdown table, ‚Äú‚â§‚ÄØ120‚ÄØwords‚Äù, etc.). Reasoning models follow detailed format guards well and will keep their lengthy analysis inside your boundaries. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91719bb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Define Chat Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4719d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment looks good: All variables are set.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not all(os.getenv(var) for var in [\"AZURE_OPENAI_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_API_VERSION\", \"REASONING_NEW\"]): \n",
    "    raise ValueError(\"‚ùå Missing one or more required env vars: Check .env.\")\n",
    "\n",
    "print(\"‚úÖ Environment looks good: All variables are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56fc0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print stats\n",
    "def print_token_and_filter_info(r):\n",
    "\n",
    "    # Print token usage\n",
    "    print(\".........................\")\n",
    "    print(\"Token Costs:\")\n",
    "    print(f\"Total Tokens: {r.usage.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {r.usage.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {r.usage.completion_tokens}\")\n",
    "    print(f\"Reasoning Tokens: {r.usage.completion_tokens_details.reasoning_tokens}\")\n",
    "    print(f\"Output Tokens: {r.usage.total_tokens - r.usage.completion_tokens_details.reasoning_tokens}\")\n",
    "    print(\".........................\")\n",
    "\n",
    "    # Print content filter results\n",
    "    '''\n",
    "    print(\"Content Filter Results:\")\n",
    "    filter_results = getattr(r.choices[0], \"content_filter_results\", None)\n",
    "    if filter_results is not None:\n",
    "        for k, v in filter_results.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No content filter results available.\")\n",
    "    print(\".........................\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default chat completion with developer persona\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "def chat(model: str = os.getenv(\"REASONING_NEW\"), persona: str=\"You are a friendly and helpful assistant.\", query: str = \"Hello!\", **kwargs):\n",
    "    r = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\":\"developer\",\"content\":persona},\n",
    "            {\"role\":\"user\",\"content\":query}\n",
    "        ],      \n",
    "        **kwargs\n",
    "    )\n",
    "    print(f\"üó£Ô∏è {model} returned:\")\n",
    "    print(r.choices[0].message.content)\n",
    "\n",
    "    # pretty print stats\n",
    "    print_token_and_filter_info(r)\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets you share history of messages and customize the persona\n",
    "def chat_with_history(messages: list, model: str=os.getenv(\"REASONING_NEW\"), query: str = \"Hello!\", **kwargs):\n",
    "    messages = messages + [{\"role\": \"user\", \"content\": query}]\n",
    "    r = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "    print(f\"üó£Ô∏è {model} returned:\")\n",
    "    print(r.choices[0].message.content)\n",
    "    print_token_and_filter_info(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3db9b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Principle 1: Developer Messages\n",
    "\n",
    "- Use a developer message (not system) - and don't mix them\n",
    "- Define a clear task scope or persona (Act as XYZ)\n",
    "- Specify an output format if it make sense (Give me a list of items)\n",
    "- Set boundaries (Use simple language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a4c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è o4-mini returned:\n",
      "Hi, friends! Today we‚Äôre going to learn about ‚Äúreasoning models.‚Äù That sounds like a big phrase, but it‚Äôs really just talking about ways our brain figures things out. Think of it like different tools or helpers we can use when we want to solve a problem or answer a question. Let‚Äôs look at three simple helpers:\n",
      "\n",
      "1. ‚ÄúStep‚Äëby‚ÄëStep‚Äù Helper  \n",
      "   ‚Ä¢ Imagine you‚Äôre making a peanut‚Äëbutter sandwich. You follow steps:  \n",
      "     1) Take two slices of bread.  \n",
      "     2) Spread peanut butter on one slice.  \n",
      "     3) Put the slices together.  \n",
      "   ‚Ä¢ When we solve a question, we can do the same‚Äîgo one small step at a time until we get our answer!\n",
      "\n",
      "2. ‚ÄúPattern Detective‚Äù Helper  \n",
      "   ‚Ä¢ A detective looks for clues and patterns. For example, you see: 2, 4, 6, 8, __.  \n",
      "   ‚Ä¢ You notice ‚Äúeach number goes up by 2.‚Äù So the next number is 10!  \n",
      "   ‚Ä¢ Our brain can be a pattern detective, too, spotting what‚Äôs the same or what comes next.\n",
      "\n",
      "3. ‚ÄúTreasure‚ÄëMap‚Äù Helper (Working Backward)  \n",
      "   ‚Ä¢ Pretend you want to find buried treasure. You know the X on the map, and you work backward from X to the starting tree.  \n",
      "   ‚Ä¢ In thinking, sometimes you start with the answer you want, then figure out each clue in reverse until you begin.\n",
      "\n",
      "Why use these helpers?  \n",
      "- They keep our thoughts neat and easy to follow.  \n",
      "- They help us check each little piece, so we don‚Äôt get lost.  \n",
      "- They make tricky problems feel simple, like a fun game!\n",
      "\n",
      "Let‚Äôs try one together:  \n",
      "Question: ‚ÄúI have 3 red apples and 2 green apples. How many apples do I have in all?‚Äù  \n",
      "\n",
      "‚Ä¢ Step‚Äëby‚ÄëStep Helper:  \n",
      "  1) Count the red apples: 3.  \n",
      "  2) Count the green apples: 2.  \n",
      "  3) Add them: 3 + 2 = 5.  \n",
      "Answer: 5 apples!\n",
      "\n",
      "Isn‚Äôt that fun? Anytime you have a question‚Äîmath, reading, or how to clean up your toys‚Äîyou can pick one of these helpers (or even all three!) and watch your brain light up with the answers. Great job learning about reasoning models today!\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 1320\n",
      "Prompt Tokens: 30\n",
      "Completion Tokens: 1290\n",
      "Reasoning Tokens: 768\n",
      "Output Tokens: 552\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "protected_material_code: {'filtered': False, 'detected': False}\n",
      "protected_material_text: {'filtered': False, 'detected': False}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# BAD PROMPT\n",
    "# Uses a system message as well as a developer message.\n",
    "# Sets no boundaries or clarity for response\n",
    "messages=[{\"role\":\"system\",\"content\":\"You are ChatGPT\"}, {\"role\":\"developer\",\"content\":\"You are a 1st grade teacher.\"}]\n",
    "query=\"Explain reasoning models\"\n",
    "test = chat_with_history(messages=messages, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cbf853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è o4-mini returned:\n",
      "Reasoning models are like treasure maps in your head:  \n",
      "They give you steps to follow, from clue A to clue Z.  \n",
      "Step by step, you never stray,  \n",
      "Finding answers along the way!\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 354\n",
      "Prompt Tokens: 37\n",
      "Completion Tokens: 317\n",
      "Reasoning Tokens: 256\n",
      "Output Tokens: 98\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# GOOD PROMPT\n",
    "# Using the system role\n",
    "# Role description is very vague\n",
    "messages=[{\"role\": \"developer\", \"content\": \"You are a 1st grade teacher. Explain concepts with analogies and rhymes. Keep answer simple ans short.\"}]\n",
    "query=\"Explain reasoning models\"\n",
    "test = chat_with_history(messages=messages, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9fdb3",
   "metadata": {},
   "source": [
    "### 2. Simple & Direct. No CoT\n",
    "\n",
    "- Keep it simple. \n",
    "- Give high-level guidance and let the model figure it out. \n",
    "- Don't offer irrelevant details - less is more\n",
    "- Don't ask it to think step by step - it internalizes chain of thought already\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Prompt** | \"Explain step by step who is lying and who is telling the truth. Show all your reasoning in detail so I can follow your chain of thought\" | \"A says ‚ÄòB is a liar.‚Äô B says ‚ÄòC is a knight.‚Äô C says nothing. Who is telling the truth?\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f955a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è o4-mini returned:\n",
      "Let‚Äôs call a person a ‚ÄúKnight‚Äù if they always tell the truth, and a ‚ÄúLiar‚Äù if they always lie.  We have three statements:\n",
      "\n",
      " 1. A says ‚ÄúB is a liar.‚Äù  \n",
      " 2. B says ‚ÄúC is a knight.‚Äù  \n",
      " 3. C says nothing.  \n",
      "\n",
      "We ask: which assignments of Knight (K) and Liar (L) to A, B, and C are logically consistent?\n",
      "\n",
      "---\n",
      "\n",
      "Step 1: Translate A‚Äôs statement.\n",
      "\n",
      "‚Äì If A is a Knight, then his statement ‚ÄúB is a liar‚Äù is true ‚áí B is L.  \n",
      "‚Äì If A is a Liar, then his statement is false ‚áí B is not a liar ‚áí B is K.  \n",
      "\n",
      "So from A we get two branches:\n",
      "  \n",
      "  Branch 1: A = K ‚áí B = L  \n",
      "  Branch 2: A = L ‚áí B = K  \n",
      "\n",
      "---\n",
      "\n",
      "Step 2: In each branch, analyze B‚Äôs statement ‚ÄúC is a knight.‚Äù\n",
      "\n",
      "Branch 1 (A=K, B=L):  \n",
      "‚Äì B is a Liar, so his statement is false.  \n",
      "‚Äì ‚ÄúC is a knight‚Äù is false ‚áí C = L.\n",
      "\n",
      "Conclusion of Branch 1:  \n",
      "  A = K, B = L, C = L  \n",
      "  (Check: A‚Äôs true statement B=L is good. B, being Liar, lies about C, so C really is L.  C says nothing‚Äîno contradiction.)\n",
      "\n",
      "Branch 2 (A=L, B=K):  \n",
      "‚Äì B is a Knight, so his statement is true.  \n",
      "‚Äì ‚ÄúC is a knight‚Äù is true ‚áí C = K.\n",
      "\n",
      "Conclusion of Branch 2:  \n",
      "  A = L, B = K, C = K  \n",
      "  (Check: A, being Liar, lies when he says ‚ÄúB is a liar,‚Äù since in fact B=K.  B truthfully says C=K.  C silent‚Äîno contradiction.)\n",
      "\n",
      "---\n",
      "\n",
      "Step 3: List the possible solutions.\n",
      "\n",
      "We have found two fully consistent assignments:\n",
      "\n",
      " 1. A = Knight, B = Liar,  C = Liar  \n",
      " 2. A = Liar,  B = Knight, C = Knight  \n",
      "\n",
      "Neither leads to a logical contradiction, and with only the given statements there is no further condition (like ‚Äúexactly one knight,‚Äù etc.) to pick just one.  \n",
      "\n",
      "Therefore the puzzle as stated admits exactly these two solutions.\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 1355\n",
      "Prompt Tokens: 66\n",
      "Completion Tokens: 1289\n",
      "Reasoning Tokens: 768\n",
      "Output Tokens: 587\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "protected_material_code: {'filtered': False, 'detected': False}\n",
      "protected_material_text: {'filtered': False, 'detected': False}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# BAD PROMPT\n",
    "# Overly verbose and forces chain-of-thought reasoning\n",
    "bad_query = (\n",
    "    \"A says ‚ÄòB is a liar.‚Äô B says ‚ÄòC is a knight.‚Äô C says nothing. Explain step by step who is lying and who is telling the truth. Show all your reasoning in detail so I can follow your chain of thought\"\n",
    ")\n",
    "test = chat(query=bad_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è o4-mini returned:\n",
      "Let A,¬†B,¬†C be ‚Äúknights‚Äù (always tell the truth) or ‚Äúliars‚Äù (always lie).  \n",
      "A says ‚ÄúB is a liar.‚Äù  \n",
      "B says ‚ÄúC is a knight.‚Äù  \n",
      "C says nothing.  \n",
      "\n",
      "Call A,¬†B,¬†C = T (knight) or F (liar).  Then truth‚Äêvalue of  \n",
      "  ‚Ä¢ A‚Äôs statement (‚ÄúB is a liar‚Äù) is [B = F].  \n",
      "  ‚Ä¢ B‚Äôs statement (‚ÄúC is a knight‚Äù) is [C = T].  \n",
      "\n",
      "We must have  \n",
      "  ‚Äì If A = T, then B = F.  \n",
      "      But then B = F ‚áí B‚Äôs statement is false ‚áí C = F.  \n",
      "      ‚áí (A,B,C) = (T,F,F) is self‚Äêconsistent.  \n",
      "  ‚Äì If A = F, then A‚Äôs statement is false ‚áí B = T.  \n",
      "      Then B = T ‚áí B‚Äôs statement is true ‚áí C = T.  \n",
      "      ‚áí (A,B,C) = (F,T,T) is also self‚Äêconsistent.  \n",
      "\n",
      "No other assignment works.  Hence there are exactly two solutions:  \n",
      " 1) A = knight, B = liar, C = liar  \n",
      " 2) A = liar,  B = knight, C = knight  \n",
      "\n",
      "In particular exactly one of A or B is telling the truth, and C turns out to be the same type as B.\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 2022\n",
      "Prompt Tokens: 44\n",
      "Completion Tokens: 1978\n",
      "Reasoning Tokens: 1664\n",
      "Output Tokens: 358\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "protected_material_code: {'filtered': False, 'detected': False}\n",
      "protected_material_text: {'filtered': False, 'detected': False}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# GOOD PROMPT\n",
    "# Simple, direct, and does not force chain-of-thought reasoning\n",
    "good_query = \"A says ‚ÄòB is a liar.‚Äô B says ‚ÄòC is a knight.‚Äô C says nothing. Who is telling the truth?\"\n",
    "test = chat(query=good_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810c09b",
   "metadata": {},
   "source": [
    "### 3‚ÄÇDelimiters for structure\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pair‚ÄØA** | ‚ÄúSummarize this plus write code‚Äù (pastes code & prose un‚Äëseparated) | ‚ÄúSummarize the prose, then improve the code in **Section‚ÄØ2** below.\\n### Section¬†1 ‚Äì¬†Prose\\n```text\\n...\\n```\\n### Section¬†2 ‚Äì¬†Code\\n```python\\n...\\n```‚Äù |\n",
    "| **Pair‚ÄØB** | ‚ÄúFix errors in my SQL:‚Äù + random HTML fragment mixed in | ‚ÄúBetween `<sql>` tags is my query; return only the corrected query.\\n<sql>\\nSELECT * FROM orders o JOIN customers c ON id;\\n</sql>‚Äù |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a57a81",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "# GOOD PROMPT using XML delimiters for structure\n",
    "query = \"\"\"\n",
    "Summarize the recipe below in 3 bullet points, focusing on key ingredients and preparation steps.\n",
    "<recipe>\n",
    "<name>Mango Margaritas</name>\n",
    "<ingredients>2 cups mango, 1 cup tequila, 1/2 cup lime juice, 1/4 cup triple sec</ingredients>\n",
    "<instructions>Blend mango, tequila, lime juice, and triple sec until smooth. Serve over ice.</instructions>\n",
    "<serving>4</serving>\n",
    "<calories>200</calories>\n",
    "<prep_time>10 minutes</prep_time>\n",
    "<total_time>10 minutes</total_time>\n",
    "<notes>Refreshing summer drink, perfect for parties!</notes>\n",
    "<tips>Use fresh mango for best flavor.</tips>\n",
    "</recipe>\n",
    "\"\"\"\n",
    "test = chat(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951dc20",
   "metadata": {},
   "source": [
    "### 4‚ÄÇRelevant context only\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pair‚ÄØA** | ‚ÄúSummarize ACME‚Äôs entire 300‚Äëpage 10‚ÄëK (pasted below) in 3 bullets.‚Äù | ‚ÄúSummarize **Risk¬†Factors** (pp‚ÄØ12‚Äë15) from ACME‚Äôs 2024¬†10‚ÄëK into 3 bullets.‚Äù |\n",
    "| **Pair‚ÄØB** | ‚ÄúBased on these 10 articles (pasted), who won the case?‚Äù | ‚ÄúUsing the quoted judgment excerpt below, identify which party (Smith¬†or¬†Jones) prevailed.\\n```<judgment>‚Ä¶</judgment>```‚Äù |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130c5ac",
   "metadata": {},
   "source": [
    "### 5‚ÄÇDecompose / iterate\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pair‚ÄØA** | ‚ÄúWrite a 40‚Äëpage business plan including market, finances, HR, legal.‚Äù | ‚ÄúStep¬†1 ‚Äî outline sections & bullet points. *Wait.*\\nStep¬†2 ‚Äî expand the **Market Analysis** section to ~500¬†words.‚Äù |\n",
    "| **Pair‚ÄØB** | ‚ÄúTranslate, summarize and turn into slides‚Äîin one go.‚Äù | ‚Äú(a) Translate the article to English.¬†¬†(b) Summarize it in 5 bullets.¬†¬†(c) Provide slide headlines.‚Äù |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685a3f4",
   "metadata": {},
   "source": [
    "### 6‚ÄÇOutput format & boundaries\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pair‚ÄØA** | ‚ÄúTell me key poll data for EU elections.‚Äù | ‚ÄúReturn JSON array `{country, pollster, sample_size, lead_pct}` for Germany, France, Spain (2024¬†polls only).‚Äù |\n",
    "| **Pair‚ÄØB** | ‚ÄúExplain transformers.‚Äù | ‚ÄúExplain transformer architecture in **exactly five bullet points of ‚â§‚ÄØ15‚ÄØwords each**, no code blocks.‚Äù |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae3c97",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "\n",
    "- OpenAI¬†Reasoning¬†Guide ‚Äì <https://platform.openai.com/docs/guides/reasoning>  \n",
    "- OpenAI¬†Reasoning Best Practices ‚Äì <https://platform.openai.com/docs/guides/reasoning-best-practices>  \n",
    "- Azure TechCommunity Blog: *Prompt Engineering for OpenAI‚Äôs O1 and O3‚Äëmini Reasoning¬†Models* ‚Äì <https://techcommunity.microsoft.com/blog/azure-ai-services-blog/prompt-engineering-for-openai%E2%80%99s-o1-and-o3-mini-reasoning-models/4374010>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
