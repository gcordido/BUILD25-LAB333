{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdcdec47",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "\n",
    "**IMPORTANT** - Run the getting started notebook first and make sure your environment is set!\n",
    "\n",
    "- Use [this table](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning?tabs=python-secure%2Cpy#api--feature-support) for the latest information on supported features. \n",
    "- Visit the [Reasoning Models](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning?tabs=python%2Cpy) documentation page for API details and code snippets. \n",
    "\n",
    "In this notebook, we'll explore capabilities like structured outputs, developer messages, reasoning effort, and vision support.- \n",
    "\n",
    "<br/>\n",
    "\n",
    "| Characteristic | o1 | o4-mini |\n",
    "|:--- |:---|:---|\n",
    "| Developer Messages    | ✅ | ✅ |\n",
    "| Structured Outputs    | ✅ | ✅ |\n",
    "| Context Window Input  | 200K | 100K |\n",
    "| Context Window Output | 200K | 100K |\n",
    "| Reasoning Effort      | ✅ | ✅ |\n",
    "| Vision Support        | ✅ | ✅ |\n",
    "| Chat Completions API  | ✅ | ✅ |\n",
    "| Responses API         | ✅ |    |\n",
    "| Functions / Tools     | ✅ | ✅ |\n",
    "| max_completion_tokens | ✅ |    |\n",
    "| System messages       | ✅ | ✅ |\n",
    "| Reasoning summary     | ✅ | |\n",
    "| Streaming             | ✅ | |\n",
    "| Model Card | [o4-mini](https://ai.azure.com/explore/models/o4-mini/version/2025-04-16/registry/azure-openai) | [o1](https://ai.azure.com/explore/models/o1/version/2024-12-17/registry/azure-openai)  |\n",
    "| api_version | 2025-04-01-preview | 2025-03-01-preview |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7b919",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ce4fb",
   "metadata": {},
   "source": [
    "## 1. Developer Messages\n",
    "\n",
    "Functionally developer messages \"role\": \"developer\" are the same as system messages.\n",
    "However, they are the recommended best practice for setting reasoning goals, persona and context.\n",
    "Don't specify both system and developer messages - that will confuse the model.\n",
    "[Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning?tabs=python-secure%2Cpy#developer-messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b04fa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a simple, well‑documented Python function that takes a list of integers and returns the sum of its even numbers:\n",
      "\n",
      "```python\n",
      "def sum_even_numbers(numbers):\n",
      "    \"\"\"\n",
      "    Compute the sum of even integers in a list.\n",
      "\n",
      "    Parameters:\n",
      "        numbers (list of int): A list of integer values.\n",
      "\n",
      "    Returns:\n",
      "        int: The sum of all even numbers in the input list. \n",
      "             If there are no even numbers, returns 0.\n",
      "\n",
      "    Example:\n",
      "        >>> sum_even_numbers([1, 2, 3, 4, 5, 6])\n",
      "        12\n",
      "        # Explanation: 2 + 4 + 6 = 12\n",
      "    \"\"\"\n",
      "    total = 0\n",
      "    for num in numbers:\n",
      "        if num % 2 == 0:\n",
      "            total += num\n",
      "    return total\n",
      "\n",
      "\n",
      "# --- Example usage ---\n",
      "if __name__ == \"__main__\":\n",
      "    sample_list = [10, 21, 32, 43, 54, 65]\n",
      "    result = sum_even_numbers(sample_list)\n",
      "    print(f\"Sum of even numbers in {sample_list} is {result}\")\n",
      "    # Output: Sum of even numbers in [10, 21, 32, 43, 54, 65] is 96\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "- We initialize a running total (`total`) to 0.\n",
      "- We iterate over each integer in the provided list.\n",
      "- If the integer is even (`num % 2 == 0`), we add it to `total`.\n",
      "- Finally, we return the computed sum.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "developer_message = \"\"\"\n",
    "You are a developer assistant. \n",
    "Your task is to help me with Python code.\n",
    "You will receive a prompt that describes the code I want to write.\n",
    "You return well documented code that is easy to understand.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a Python function that takes a list of integers and returns the sum of the even numbers in the list.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_level = \"low\"  # Options: low, medium, high\n",
    "\n",
    "\n",
    "# Set up the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": developer_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_completion_tokens=5000,\n",
    "    reasoning_effort=reasoning_level\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22c63e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Structured Inputs\n",
    "\n",
    "Structured outputs make a model follow a JSON Schema definition that you provide as part of your inference API call. This is in contrast to the older JSON mode feature, which guaranteed valid JSON would be generated, but was unable to ensure strict adherence to the supplied schema. Structured outputs are recommended for function calling, extracting structured data, and building complex multi-step workflows. [Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python%2Cdotnet-keys&pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb8c2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "# Define a Pydantic model to represent the event information\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "# Define a custom parser for the chat completion response\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"o4-mini\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "# Get the parsed event information\n",
    "event = completion.choices[0].message.parsed\n",
    "print(event)\n",
    "#print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e73cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Function Calling \n",
    "\n",
    "- Structured Outputs for function calling can be enabled with a single parameter, by supplying strict: true.\n",
    "- Structured outputs are not supported with parallel function calls. When using structured outputs set parallel_tool_calls to false.\n",
    "- [Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python%2Cdotnet-entra-id&pivots=programming-language-python#function-calling-with-structured-outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8040bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"order_id\":\"12345\"}', name='GetDeliveryDate')\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "class GetDeliveryDate(BaseModel):\n",
    "    order_id: str\n",
    "\n",
    "tools = [openai.pydantic_function_tool(GetDeliveryDate)]\n",
    "\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"You are a helpful customer support assistant. Use the supplied tools to assist the user.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Hi, can you tell me the delivery date for my order #12345?\"}) \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\", \n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.tool_calls[0].function)\n",
    "#print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ca4fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.a. Vision - Identify\n",
    "Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. [Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=rest)\n",
    "\n",
    "Let's try an image from [the Library of Congress](https://www.loc.gov/free-to-use/lighthouses/) and see if it can reason about the location.\n",
    "\n",
    "<img src=\"https://www.loc.gov/static/portals/free-to-use/public-domain/lighthouses/lighthouses-2.jpg\" alt=\"NYPL\" width=\"20%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That’s the Cape Hatteras Lighthouse on Hatteras Island in North Carolina’s Outer Banks.  Key points:\n",
      "\n",
      "• Appearance: 207‑foot‑tall brick tower painted in a distinctive black‑and‑white spiral.  \n",
      "• History: Completed in 1870 to warn ships off Diamond Shoals, once the “Graveyard of the Atlantic.”  \n",
      "• Relocation: In 1999, it was moved intact about 2,900 feet inland to protect it from shoreline erosion.  \n",
      "• National Park Service: Part of Cape Hatteras National Seashore; open for public tours (climbing the 257 steps to the gallery).  \n",
      "• Landmark status: Tallest brick lighthouse in the U.S. and one of the most‑photographed lighthouses on the Atlantic coast.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Can you recognize the location? Tell me about it.\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://www.loc.gov/static/portals/free-to-use/public-domain/lighthouses/lighthouses-2.jpg\"\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_completion_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00131ec2",
   "metadata": {},
   "source": [
    "## 4.b Vision - Decipher\n",
    "\n",
    "Let's try an image from [the Library of Congress](https://www.loc.gov/static/portals/free-to-use/public-domain/presidential-papers) and see if it can recognize handwriting and reason about it.\n",
    "\n",
    "<img src=\"https://www.loc.gov/static/portals/free-to-use/public-domain/presidential-papers/2.jpg\" alt=\"NYPL\" width=\"20%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec9b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the manuscript of Abraham Lincoln’s Gettysburg Address (November 19, 1863). It’s famous because:  \n",
      "1. It’s astonishingly brief—just about 270 words—but powerfully distilled.  \n",
      "2. Lincoln redefined the Civil War as a fight not merely for Union, but for “a new birth of freedom” and true equality (“all men are created equal”).  \n",
      "3. It consecrated the Gettysburg battlefield as hallowed ground and honored the fallen in a deeply moving way.  \n",
      "4. Its rhetoric (“government of the people, by the people, for the people”) has become a foundational statement of American democracy.  \n",
      "5. It transformed public memory of the war and endures as one of the most quoted speeches in U.S. history.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Why is this famous? Summarize it in a short list.\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://www.loc.gov/static/portals/free-to-use/public-domain/presidential-papers/2.jpg\"\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_completion_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b5f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visual - Local Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091616e4",
   "metadata": {},
   "source": [
    "\n",
    "If you want to use a local image, you can use the following Python code to convert it to base64 so it can be passed to the API. Alternative file conversion tools are available online. Learn more about this and [other settings](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=python#detail-parameter-settings). Let's try an example with this local image.\n",
    "\n",
    "<img src=\"./assets/lavacake.webp\" alt=\"Lava Cake\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ccd62da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a rough breakdown of what I see on the plate and their approximate calories:\n",
      "\n",
      "• Molten chocolate cake (6–8 cm diameter)  \n",
      "  – about 300–400 kcal  \n",
      "• Single scoop vanilla ice cream (≈½ cup)  \n",
      "  – about 150–180 kcal  \n",
      "• Berry coulis or compote (a couple of tablespoons)  \n",
      "  – about 30–60 kcal  \n",
      "• Sliced strawberries (3–4 berries)  \n",
      "  – about 10–15 kcal  \n",
      "• Black or lightly sweetened tea (mug)  \n",
      "  – negligible to 5 kcal  \n",
      "\n",
      "Adding these up gives a total in the neighborhood of 500–660 kcal. Of course, actual values will vary with the exact recipe, portion sizes, and any hidden fats or sugars, so consider this a ballpark estimate.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Transforming local image to base64\n",
    "image_path = './assets/lavacake.webp'\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "image_uri = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "\n",
    "# Make a request to the o4-mini model with the image\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"How many calories are in this ?\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": image_uri,\n",
    "                    \"detail\" : \"high\"\n",
    "                }       \n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_completion_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f474605",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34de4f1",
   "metadata": {},
   "source": [
    "## 6. Your Turn - Try It!\n",
    "\n",
    "Copy one of the above examples and modify it to see how the model responds to different prompts or images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your developer message\n",
    "developer_message = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Write your prompt\n",
    "prompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Write your reasoning level\n",
    "reasoning_level = \"low\"  \n",
    "\n",
    "# ---------- Run the cell to see result ------\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": developer_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_completion_tokens=5000,\n",
    "    reasoning_effort=reasoning_level\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)\n",
    "# --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed726ae",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
